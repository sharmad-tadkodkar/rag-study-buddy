import os
import streamlit as st

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters.sentence_transformers import SentenceTransformersTokenTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from st_copy_to_clipboard import st_copy_to_clipboard
from dotenv import load_dotenv

load_dotenv(override=True)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")


# Initialize embedding model
embedding_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GEMINI_API_KEY)

# Initialize study database
db = Chroma(collection_name="study_database",
            embedding_function=embedding_model,
            persist_directory='./study_db')

def format_docs(docs):
    """Formats a list of document objects into a single string.

    Args:
        docs (list): A list of document objects, each having a 'page_content' attribute.

    Returns:
        str: A single string containing the page content from each document, 
        separated by double newlines."""
    return "\n\n".join(doc.page_content for doc in docs)

def add_to_db(uploaded_files):
    """Processes and adds uploaded PDF files to the database.

    This function checks if any files have been uploaded. If files are uploaded,
    it saves each file to a temporary location, processes the content using a PDF loader,
    and splits the content into smaller chunks. These chunks are then added to the 
    database in batches to avoid timeout errors. Temporary files are removed after processing.

    Args:
        uploaded_files (list): A list of uploaded file objects to be processed.

    Returns:
        None"""
    # Check if files are uploaded
    if not uploaded_files:
        st.error("No files uploaded!")
        return

    for uploaded_file in uploaded_files:
        # Save the uploaded file to a temporary path
        temp_file_path = os.path.join("./temp", uploaded_file.name)
        os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)

        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(uploaded_file.getbuffer())

        # Load the file using PyPDFLoader
        loader = PyPDFLoader(temp_file_path)
        data = loader.load()

        # Store metadata and content
        doc_metadata = [data[i].metadata for i in range(len(data))]
        doc_content = [data[i].page_content for i in range(len(data))]

        # Split documents into smaller chunks
        st_text_splitter = SentenceTransformersTokenTextSplitter(
            model_name="sentence-transformers/all-mpnet-base-v2",
            chunk_size=100,
            chunk_overlap=50
        )
        st_chunks = st_text_splitter.create_documents(doc_content, doc_metadata)

        # Add chunks to database in batches to avoid timeout errors
        batch_size = 100  # You can adjust this value
        for i in range(0, len(st_chunks), batch_size):
            batch = st_chunks[i:i + batch_size]
            db.add_documents(batch)
            # Optional: Add a small delay between batches if you still face issues
            # import time; time.sleep(1)

        # Remove the temporary file after processing
        os.remove(temp_file_path)

def run_rag_chain(query):
    """Processes a query using a Retrieval-Augmented Generation (RAG) chain.

    This function utilizes a RAG chain to answer a given query. It retrieves 
    relevant context using similarity search and then generates a response 
    based on this context using a chat model. The chat model is pre-configured 
    with a prompt template catered to solve doubts.

    Args:
        query (str): The user's question that needs to be answered.

    Returns:
        str: A response generated by the chat model, based on the retrieved context."""
    # Create a Retriever Object and apply Similarity Search
    retriever = db.as_retriever(search_type="similarity", search_kwargs={'k': 5})

    # Initialize a Chat Prompt Template
    PROMPT_TEMPLATE = """
    You are StudyBuddy, an AI tutor designed for school and college students.
    Your purpose is to explain academic concepts clearly, help students understand their materials, and answer questions 
    based only on the following context:
    {context}

    Answer the question based on the above context:
    {question}

    Use the provided context to answer the user's question accurately and concisely.
    Don't justify your answers.
    Don't give information not mentioned in the CONTEXT INFORMATION. Never hallucinate document facts or cite information that was not provided.
    Do not say "according to the context" or "mentioned in the context" or similar.
    Use bullet points or steps for explanations.
    Provide examples wherever possible.
    Break complex ideas into simpler concepts.
    
    When answering:
    Identify the student's question and what level they are at (school/college).
    Examine CONTEXT thoroughly and extract key points.
    Provide:
        -Definition 
        -Key ideas
        -Step-by-step breakdown
        -Real-world analogy (if helpful)
        -Optional diagram/video/flowchart generation
    """

    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

    # Initialize a Generator (i.e. Chat Model)
    chat_model = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        api_key=st.session_state.get("gemini_api_key"),
        temperature=0
    )

    # Initialize a Output Parser
    output_parser = StrOutputParser()

    # RAG Chain
    rag_chain = {"context": retriever | format_docs, "question": RunnablePassthrough()} | prompt_template | chat_model | output_parser

    # Invoke the Chain
    response = rag_chain.invoke(query)

    return response

def main():
    """Initialize and manage the StudyBuddy application interface.

    This function sets up the Streamlit application interface for StudyBuddy,
    a Educational Insight Retrieval System. Users can enter queries related
    to the study topic, upload research documents, and manage API 
    keys for enhanced functionality.

    The main features include:
    - Query input area for users to ask questions about the study topic.
    - Submission button to process the query and display the retrieved insights.
    - Sidebar for API key input and management.
    - File uploader for adding research documents to the database, enhancing query responses.

    Args:
        None

    Returns:
        None"""
    st.set_page_config(page_title="StudyBuddy", page_icon=":microscope:")
    st.header("Study Insight Retrieval System")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Hello! I'm StudyBuddy. How can I help you with your studies today?"}
        ]

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Accept user input
    if prompt := st.chat_input("What is your question?"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(prompt)

        # Display assistant response in chat message container
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = run_rag_chain(prompt)
                st.write(response)
                st_copy_to_clipboard(response, key="copy_response")
                

        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})
    
    # with st.sidebar:
    #     st.title("API Keys")
    #     # gemini_api_key = st.text_input("Enter your Gemini API key:", type="password")
        

    #     if st.button("Enter"):
    #         if gemini_api_key:
    #             st.session_state.gemini_api_key = gemini_api_key
    #             st.success("API key saved!")

    #         else:
    #             st.warning("Please enter your Gemini API key to proceed.")
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if gemini_api_key:
        st.session_state.gemini_api_key = gemini_api_key
        
    else:
        st.warning("Please enter your Gemini API key to proceed.")
    

    with st.sidebar:
        st.markdown("---")
        pdf_docs = st.file_uploader("Upload your Study materials (Optional) :memo:",
                                    type=["pdf"],
                                    accept_multiple_files=True
        )
        
        if st.button("Submit & Process"):
            if not pdf_docs:
                st.warning("Please upload the file")

            else:
                with st.spinner("Processing your documents..."):
                    add_to_db(pdf_docs)
                    st.success(":file_folder: Documents successfully added to the database!")

    # Sidebar Footer
    st.sidebar.write("Built by [Sharmad](www.linkedin.com/in/sharmad-tadkodkar-40b20314b)")
             
if __name__ == "__main__":
    main()